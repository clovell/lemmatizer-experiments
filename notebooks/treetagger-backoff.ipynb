{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from nltk.tag.sequential import SequentialBackoffTagger\n",
    "\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "\n",
    "from treetagger import TreeTagger\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeTaggerLemmatizer(SequentialBackoffTagger):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, backoff=None):\n",
    "        \"\"\"Setup for TreeTaggerLemmatizer().\"\"\"\n",
    "        \n",
    "        SequentialBackoffTagger.__init__(self, backoff)\n",
    "        self.tagger = TreeTagger(language='latin') # Error trap to see if module is installed!\n",
    "        \n",
    "        \n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        \"\"\"Returns the lemma tagged in lemmatize by TreeTagger.\n",
    "        :param tokens: List of tokens to be lemmatized\n",
    "        :param index: Int with current token\n",
    "        :param history: List with tokens that have already been lemmatized\n",
    "        :return: String, spec. the token found at the current index.\n",
    "        \"\"\"\n",
    "        print(f'Choosing {tokens[index]}')\n",
    "        if tokens[index]:\n",
    "            return tokens[index]    \n",
    "    \n",
    "    \n",
    "    def tag(self, tokens):\n",
    "        \"\"\"\n",
    "        Backoff Lemmatizer wrapper for TreeTagger\n",
    "        # Note: only takes the first match (for now!) in returned lemma, \n",
    "        # i.e. 'vir|virum|virus|vis' is truncated to 'vir'. Not ideal.\n",
    "        :param tokens: List of tokens to be lemmatized\n",
    "        :return: Tuple of the form (TOKEN, LEMMA)\n",
    "        \"\"\"\n",
    "        lemmas = []\n",
    "        text = \" \".join(tokens)\n",
    "        lemmas = []\n",
    "        for _, _, lemma in self.tagger.tag(text):\n",
    "            if lemma == '<unknown>':\n",
    "                lemmas.append(None)\n",
    "            else:\n",
    "                lemmas.append(lemma.split('|')[0])\n",
    "        print(list(zip(tokens,lemmas)))\n",
    "        return list(zip(tokens, lemmas))\n",
    "    \n",
    "#     def lemmatize(self, tokens):\n",
    "#         \"\"\"\n",
    "#         Backoff Lemmatizer wrapper for TreeTagger\n",
    "#         # Note: only takes the first match (for now!) in returned lemma, \n",
    "#         # i.e. 'vir|virum|virus|vis' is truncated to 'vir'. Not ideal.\n",
    "#         :param tokens: List of tokens to be lemmatized\n",
    "#         :return: Tuple of the form (TOKEN, LEMMA)\n",
    "#         \"\"\"\n",
    "#         lemmas = []\n",
    "#         text = \" \".join(tokens)\n",
    "#         lemmas = []\n",
    "#         for _, _, lemma in self.tagger.tag(text):\n",
    "#             if lemma == '<unknown>':\n",
    "#                 lemmas.append(None)\n",
    "#             else:\n",
    "#                 lemmas.append(lemma.split('|')[0])\n",
    "#         print(list(zip(tokens,lemmas)))\n",
    "#         return list(zip(tokens, lemmas))\n",
    "\n",
    "    def lemmatize(self, tokens):\n",
    "        return self.tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TreeTaggerLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Omnis homines qui sese student praestare ceteris animalibus, summa ope niti decet, ne vitam silentio transeant veluti pecora, quae natura prona atque ventri oboedientia finxit. Sed nostra omnis vis in animo et corpore sita est: animi imperio, corporis servitio magis utimur; alterum nobis cum dis, alterum cum beluis commune est. Quo mihi rectius videtur ingeni quam virium opibus gloriam quaerere et, quoniam vita ipsa, qua fruimur, brevis est, memoriam nostri quam maxume longam efficere. Nam divitiarum et formae gloria fluxa atque fragilis est, virtus clara aeternaque habetur. Sed diu magnum inter mortalis certamen fuit, vine corporis an virtute animi res militaris magis procederet. Nam et, prius quam incipias, consulto et, ubi consulueris, mature facto opus est. Ita utrumque per se indigens alterum alterius auxilio eget.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Omnis', 'omnis'), ('homines', 'homo'), ('qui', 'qui'), ('sese', 'sui'), ('student', 'studeo'), ('praestare', 'praesto'), ('ceteris', 'ceterus'), ('animalibus', 'animal'), (',', ','), ('summa', 'summus'), ('ope', 'ops'), ('niti', 'nitor'), ('decet', 'decet'), (',', ','), ('ne', 'ne'), ('vitam', 'vita'), ('silentio', 'silentium'), ('transeant', 'transeo'), ('veluti', 'veluti'), ('pecora', 'pecus'), (',', ','), ('quae', 'qui'), ('natura', 'natura'), ('prona', 'pronus'), ('atque', 'atque'), ('ventri', 'venter'), ('oboedientia', 'oboedio'), ('finxit', 'fingo'), ('.', '.'), ('Sed', 'sed'), ('nostra', 'noster'), ('omnis', 'omnis'), ('vis', 'vis'), ('in', 'in'), ('animo', 'animus'), ('et', 'et'), ('corpore', 'corpus'), ('sita', 'sino'), ('est', 'sum'), (':', ':'), ('animi', 'animus'), ('imperio', 'imperium'), (',', ','), ('corporis', 'corpus'), ('servitio', 'servitium'), ('magis', 'magis'), ('utimur', 'utor'), (';', ';'), ('alterum', 'alter'), ('nobis', 'nos'), ('cum', 'cum'), ('dis', 'deus'), (',', ','), ('alterum', 'alter'), ('cum', 'cum'), ('beluis', 'belua'), ('commune', 'communis'), ('est', 'sum'), ('.', '.'), ('Quo', 'qui'), ('mihi', 'ego'), ('rectius', 'rectus'), ('videtur', 'video'), ('ingeni', 'ingeno'), ('quam', 'qui'), ('virium', 'vis'), ('opibus', 'ops'), ('gloriam', 'gloria'), ('quaerere', 'quaero'), ('et', 'et'), (',', ','), ('quoniam', 'quoniam'), ('vita', 'vita'), ('ipsa', 'ipse'), (',', ','), ('qua', 'qui'), ('fruimur', 'fruor'), (',', ','), ('brevis', 'brevis'), ('est', 'sum'), (',', ','), ('memoriam', 'memoria'), ('nostri', 'noster'), ('quam', 'qui'), ('maxume', 'maxume'), ('longam', 'longus'), ('efficere', 'efficio'), ('.', '.'), ('Nam', 'nam'), ('divitiarum', 'divitia'), ('et', 'et'), ('formae', 'forma'), ('gloria', 'gloria'), ('fluxa', 'fluo'), ('atque', 'atque'), ('fragilis', 'fragilis'), ('est', 'sum'), (',', ','), ('virtus', 'virtus'), ('clara', 'clarus'), ('aeterna', 'aeternus'), ('-que', None), ('habetur', 'habeo'), ('.', '.'), ('Sed', 'sed'), ('diu', 'diu'), ('magnum', 'magnus'), ('inter', 'inter'), ('mortalis', 'mortalis'), ('certamen', 'certamen'), ('fuit', 'sum'), (',', ','), ('vine', 'vis'), ('corporis', 'corpus'), ('an', 'an'), ('virtute', 'virtus'), ('animi', 'animus'), ('res', 'res'), ('militaris', 'militaris'), ('magis', 'magis'), ('procederet', 'procedo'), ('.', '.'), ('Nam', 'nam'), ('et', 'et'), (',', ','), ('prius', 'prius'), ('quam', 'qui'), ('incipias', 'incipio'), (',', ','), ('consulto', 'consultum'), ('et', 'et'), (',', ','), ('ubi', 'ubi'), ('consulueris', 'consulo'), (',', ','), ('mature', 'mature'), ('facto', 'facio'), ('opus', 'opus'), ('est', 'sum'), ('.', '.'), ('Ita', 'ita'), ('utrumque', 'uterque'), ('per', 'per'), ('se', 'se'), ('indigens', 'indigeo'), ('alterum', 'alter'), ('alterius', 'alter'), ('auxilio', 'auxilium'), ('eget', 'egeo'), ('.', '.')]\n",
      "CPU times: user 17.2 ms, sys: 8.1 ms, total: 25.3 ms\n",
      "Wall time: 2.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lemma_pairs = t.lemmatize(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Omnis', 'omnis'),\n",
      " ('homines', 'homo'),\n",
      " ('qui', 'qui'),\n",
      " ('sese', 'sui'),\n",
      " ('student', 'studeo'),\n",
      " ('praestare', 'praesto'),\n",
      " ('ceteris', 'ceterus'),\n",
      " ('animalibus', 'animal'),\n",
      " (',', ','),\n",
      " ('summa', 'summus')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lemma_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"\"\" Est brilgum: tovi slimici\n",
    "# In vabo tererotitant\n",
    "# Brogovi sunt macresculi\n",
    "# Momi rasti strugitant.\n",
    "\n",
    "# \"Fuge Gabrobocchia, fili mi,\n",
    "# Qui fero lacerat morsu:\n",
    "# Diffide Iubiubae avi\n",
    "# Es procul ab Unguimanu.\"\"\"\n",
    "\n",
    "text = \"\"\"macresculi\n",
    "Momi rasti strugitant.\n",
    "\n",
    "\"Fuge Gabrobocchia, fili mi,\n",
    "Qui fero lacerat morsu:\n",
    "Diffide Iubiubae avi\n",
    "Es procul ab Unguimanu.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('macresculi', None), ('Momi', None), ('rasti', None), ('strugitant', None), ('.', '.'), ('\"', '\"'), ('Fuge', 'fugio'), ('Gabrobocchia', None), (',', ','), ('fili', 'filius'), ('mi', 'meus'), (',', ','), ('Qui', None), ('fero', 'ferus'), ('lacerat', 'lacero'), ('morsu', 'morsus'), (':', ':'), ('Diffide', 'diffido'), ('Iubiubae', None), ('avi', 'avis'), ('Es', 'edo'), ('procul', 'procul'), ('ab', 'ab'), ('Unguimanu', None), ('.', '.')]\n",
      "[('macresculi', None),\n",
      " ('Momi', None),\n",
      " ('rasti', None),\n",
      " ('strugitant', None),\n",
      " ('.', '.'),\n",
      " ('\"', '\"'),\n",
      " ('Fuge', 'fugio'),\n",
      " ('Gabrobocchia', None),\n",
      " (',', ','),\n",
      " ('fili', 'filius')]\n"
     ]
    }
   ],
   "source": [
    "lemma_pairs = t.lemmatize(tokenizer.tokenize(text))\n",
    "pprint(lemma_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.lemmatize.backoff import UnigramLemmatizer, RegexpLemmatizer\n",
    "from cltk.lemmatize.latin.latin import latin_sub_patterns\n",
    "\n",
    "# u = UnigramLemmatizer(model={'rastus': 'rasti'})\n",
    "backoff = RegexpLemmatizer(latin_sub_patterns, backoff=None)\n",
    "lemmatizer = TreeTaggerLemmatizer(backoff=backoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('macresculi', None), ('Momi', None), ('rasti', None), ('strugitant', None), ('.', '.'), ('\"', '\"'), ('Fuge', 'fugio'), ('Gabrobocchia', None), (',', ','), ('fili', 'filius'), ('mi', 'meus'), (',', ','), ('Qui', None), ('fero', 'ferus'), ('lacerat', 'lacero'), ('morsu', 'morsus'), (':', ':'), ('Diffide', 'diffido'), ('Iubiubae', None), ('avi', 'avis'), ('Es', 'edo'), ('procul', 'procul'), ('ab', 'ab'), ('Unguimanu', None), ('.', '.')]\n",
      "[('macresculi', None),\n",
      " ('Momi', None),\n",
      " ('rasti', None),\n",
      " ('strugitant', None),\n",
      " ('.', '.'),\n",
      " ('\"', '\"'),\n",
      " ('Fuge', 'fugio'),\n",
      " ('Gabrobocchia', None),\n",
      " (',', ','),\n",
      " ('fili', 'filius'),\n",
      " ('mi', 'meus'),\n",
      " (',', ','),\n",
      " ('Qui', None),\n",
      " ('fero', 'ferus'),\n",
      " ('lacerat', 'lacero'),\n",
      " ('morsu', 'morsus'),\n",
      " (':', ':'),\n",
      " ('Diffide', 'diffido'),\n",
      " ('Iubiubae', None),\n",
      " ('avi', 'avis'),\n",
      " ('Es', 'edo'),\n",
      " ('procul', 'procul'),\n",
      " ('ab', 'ab'),\n",
      " ('Unguimanu', None),\n",
      " ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "lemma_pairs = lemmatizer.lemmatize(tokenizer.tokenize(text))\n",
    "pprint(lemma_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "tagger1 = RegexpLemmatizer(latin_sub_patterns, backoff=None)\n",
    "tagger2 = TreeTaggerLemmatizer(backoff=tagger1)\n",
    "print(tagger1._taggers == [tagger1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tagger2._taggers == [tagger2, tagger1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Regexp Tagger: size=18>]\n",
      "[<__main__.TreeTaggerLemmatizer object at 0x10a210390>, <Regexp Tagger: size=18>]\n"
     ]
    }
   ],
   "source": [
    "print(tagger1._taggers)\n",
    "print(tagger2._taggers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Regexp Tagger: size=18>\n",
      "<__main__.TreeTaggerLemmatizer object at 0x10a210390>\n"
     ]
    }
   ],
   "source": [
    "print(tagger1)\n",
    "print(tagger2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
