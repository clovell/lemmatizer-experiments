{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TreeTagger-based Backoff Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what a [TreeTagger](http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/)-based sublemmatizer for the CLTK Latin Backoff Lemmatizer would look like. It's something like a wrapper wrapper—that is, it creates a subclass of the sequential backoff tagger that gets lemma information from the *treetagger-python* wrapper. The ```lemmatize``` method takes a list of tokens (like all of the CLTK backoff lemmatizers), joins them, and runs this string through TreeTagger. The Backoff Lemmatizer works by trying to return a lemma for a given token with one lemmatizer and, if no match is found (i.e. ```None``` is returned), it backs off to a second lemmatizer (and a third, etc.). Accordingly, with the TreeTaggerLemmatizer, ```'<unknown'>``` is replaced with ```None``` so that the backoff tagger can work properly.\n",
    "\n",
    "When used on its own, the results are what you would expect from [TreeTagger](https://github.com/diyclassics/lemmatizer-experiments/blob/master/notebooks/latin-lemmatization-with-treetagger.ipynb). This is shown in the first example below using the first paragraph of Sallust's *Bellum Catilinae*.\n",
    "\n",
    "The next two examples—using the nonsense Latin text of a [*Jabberwocky* translation](http://www.thelatinlibrary.com/iabervocius.html)—shows the flexibility of the Backoff Lemmatizer and how the TreeTaggerLemmatizer results can improved upon.\n",
    "\n",
    "In the first example, we create a chain of TreeTaggerLemmatizer and RegexpLemmatizer and see that a nonsense word like *vabo* is reasonably(?) lemmatized to *vo*, because the Latin regex substitution patterns include *-abo > -o*. *Vabo* is the hapaxest of hapax legomena—it has never appeared in another Latin text and so will not be found in any lemma dictionary. (It is also admittedly wrong—*vabo* here is clearly the ablative of a noun *vabus* or *vabum* in the phrase *in vabo*. We will fix this in a future post by combining lemmatization with POS-tagging.)\n",
    "\n",
    "The next example, expands the backoff chain to include additional regex patterns and a custom lemma dictionary (using the UnigramLemmatizer) to lemmatize fully the first ten tokens in the *Jabberwocky*.\n",
    "\n",
    "The TreeTaggerLemmatizer works well and I plan to introduce it to the CLTK with the next update to the ```lemmatize``` class. It, of course, requires TreeTagger to be installed as well as the *treetagger-python* package.\\[PJB 5.6.18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from nltk.tag.sequential import SequentialBackoffTagger\n",
    "\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "\n",
    "from treetagger import TreeTagger\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of CLTK tools\n",
    "\n",
    "tokenizer = WordTokenizer('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TreeTaggerLemmatizer as subclass of NLTK's Sequential Backoff Tagger\n",
    "\n",
    "class TreeTaggerLemmatizer(SequentialBackoffTagger):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, backoff=None):\n",
    "        \"\"\"Setup for TreeTaggerLemmatizer().\"\"\"\n",
    "        SequentialBackoffTagger.__init__(self, backoff)\n",
    "        self.tagger = TreeTagger(language='latin') # Error trap to see if module is installed!\n",
    "        self._lemmas = []\n",
    "        \n",
    "        \n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        \"\"\"Returns the lemma at the index in the _lemmas list created\n",
    "        by TreeTagger in lemmatize.\n",
    "        :param tokens: List of tokens to be lemmatized\n",
    "        :param index: Int with current token\n",
    "        :param history: List with tokens that have already been lemmatized\n",
    "        :return: String, spec. the lemma found at the current index.\n",
    "        \"\"\"\n",
    "        return self._lemmas[index]    \n",
    "    \n",
    "    def lemmatize(self, tokens):\n",
    "        lemmas = []\n",
    "        text = \" \".join([token.lower() for token in tokens])\n",
    "        lemmas = []\n",
    "        for _, _, lemma in self.tagger.tag(text):\n",
    "            if lemma == '<unknown>':\n",
    "                lemmas.append(None)\n",
    "            else:\n",
    "                lemmas.append(lemma.split('|')[0])\n",
    "        self._lemmas = lemmas\n",
    "        return self.tag(tokens)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of lemmatizer\n",
    "\n",
    "lemmatizer = TreeTaggerLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "\n",
    "text = \"\"\"Omnis homines qui sese student praestare ceteris animalibus, summa ope niti decet, ne vitam silentio transeant veluti pecora, quae natura prona atque ventri oboedientia finxit. Sed nostra omnis vis in animo et corpore sita est: animi imperio, corporis servitio magis utimur; alterum nobis cum dis, alterum cum beluis commune est. Quo mihi rectius videtur ingeni quam virium opibus gloriam quaerere et, quoniam vita ipsa, qua fruimur, brevis est, memoriam nostri quam maxume longam efficere. Nam divitiarum et formae gloria fluxa atque fragilis est, virtus clara aeternaque habetur. Sed diu magnum inter mortalis certamen fuit, vine corporis an virtute animi res militaris magis procederet. Nam et, prius quam incipias, consulto et, ubi consulueris, mature facto opus est. Ita utrumque per se indigens alterum alterius auxilio eget.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.6 ms, sys: 16.1 ms, total: 50.7 ms\n",
      "Wall time: 6.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get lemmas\n",
    "\n",
    "lemma_pairs = lemmatizer.lemmatize(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Omnis', 'omnis'),\n",
      " ('homines', 'homo'),\n",
      " ('qui', 'qui'),\n",
      " ('sese', 'sui'),\n",
      " ('student', None),\n",
      " ('praestare', 'praesto'),\n",
      " ('ceteris', 'ceterus'),\n",
      " ('animalibus', 'animal'),\n",
      " (',', ','),\n",
      " ('summa', 'summus')]\n"
     ]
    }
   ],
   "source": [
    "# Print sample\n",
    "\n",
    "pprint(lemma_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another sample text\n",
    "\n",
    "text = \"\"\" Est brilgum: tovi slimici\n",
    "In vabo tererotitant\n",
    "Brogovi sunt macresculi\n",
    "Momi rasti strugitant.\n",
    "\n",
    "\"Fuge Gabrobocchia, fili mi,\n",
    "Qui fero lacerat morsu:\n",
    "Diffide Iubiubae avi\n",
    "Es procul ab Unguimanu.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.78 ms, sys: 13.1 ms, total: 22.9 ms\n",
      "Wall time: 7.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get lemmas\n",
    "\n",
    "lemma_pairs = lemmatizer.lemmatize(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Est', 'sum'),\n",
      " ('brilgum', None),\n",
      " (':', ':'),\n",
      " ('tovi', None),\n",
      " ('slimici', None),\n",
      " ('In', 'in'),\n",
      " ('vabo', None),\n",
      " ('tererotitant', None),\n",
      " ('Brogovi', None),\n",
      " ('sunt', 'sum')]\n"
     ]
    }
   ],
   "source": [
    "# Print sample\n",
    "\n",
    "pprint(lemma_pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that only words like 'est' and 'in' (and punctuation) are lemmatized on a first pass. These are of course the functional vocabulary necessary to keep the nonsense Latin recognizable as Latin at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional lemmatizers/resources\n",
    "\n",
    "from cltk.lemmatize.backoff import UnigramLemmatizer, RegexpLemmatizer\n",
    "from cltk.lemmatize.latin.latin import latin_sub_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up lemmatizer with backoff chain\n",
    "\n",
    "backoff = RegexpLemmatizer(latin_sub_patterns, backoff=None)\n",
    "lemmatizer = TreeTaggerLemmatizer(backoff=backoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 ms, sys: 11.3 ms, total: 22.9 ms\n",
      "Wall time: 7.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get lemmas\n",
    "\n",
    "lemma_pairs = lemmatizer.lemmatize(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Est', 'sum'),\n",
      " ('brilgum', None),\n",
      " (':', ':'),\n",
      " ('tovi', None),\n",
      " ('slimici', None),\n",
      " ('In', 'in'),\n",
      " ('vabo', 'vo'),\n",
      " ('tererotitant', None),\n",
      " ('Brogovi', None),\n",
      " ('sunt', 'sum')]\n"
     ]
    }
   ],
   "source": [
    "# Print sample\n",
    "\n",
    "pprint(lemma_pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the introduction of the RegexpLemmatizer to the backoff chain has now returned the result *vo* for *vabo*. \\(As I mention in the introduction, this is an *incorrect* result, but a result nonetheless)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a more expansive backoff chain\n",
    "\n",
    "r = RegexpLemmatizer([\n",
    "    ('(.)(ant)$', '\\\\1o'), \n",
    "    ('(.)(um)$', '\\\\1us'),\n",
    "    ('(.)(i)$', '\\\\1us')\n",
    "])\n",
    "u = UnigramLemmatizer(model={'Momi': 'momus'}, backoff=r)\n",
    "backoff = RegexpLemmatizer(latin_sub_patterns, backoff=u)\n",
    "lemmatizer = TreeTaggerLemmatizer(backoff=backoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 ms, sys: 12.6 ms, total: 24.4 ms\n",
      "Wall time: 6.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get lemmas\n",
    "\n",
    "lemma_pairs = lemmatizer.lemmatize(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Est', 'sum'),\n",
      " ('brilgum', 'brilgus'),\n",
      " (':', ':'),\n",
      " ('tovi', 'tovus'),\n",
      " ('slimici', 'slimicus'),\n",
      " ('In', 'in'),\n",
      " ('vabo', 'vo'),\n",
      " ('tererotitant', 'tererotito'),\n",
      " ('Brogovi', 'Brogovus'),\n",
      " ('sunt', 'sum')]\n"
     ]
    }
   ],
   "source": [
    "# Print sample\n",
    "\n",
    "pprint(lemma_pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all ten of the first ten words of the *Jabberwocky* translation have been lemmatized, plausibly, if not correctly (whatever correctly means in lemmatizing nonsense poetry.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
