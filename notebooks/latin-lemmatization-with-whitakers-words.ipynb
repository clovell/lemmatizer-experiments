{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latin Lemmatization with Whitaker's Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Whitaker's *Words*](http://archives.nd.edu/whitaker/words.htm) is command-line dictionary for Latin written in Ada by William Whitaker in 1993. It is a beloved digital Latin resource with several unofficial implementations, ports, etc. This notebook can be considered yet another example. It is also the focus of a digital preservation effort by Martin Keegan, who is hosting and maintaining the original code on [GitHub](https://github.com/mk270/whitakers-words).\n",
    "\n",
    "There is no Python wrapper that I could find for *Words*. (I did find this in-progress [Python port](https://github.com/ArchimedesDigital/open_words) by Archimedes Digital, but found the results too uneven at this point to publish. See Appendix B of this Notebook.) This notebook moves in that direction, using ```subprocess``` to write results to a temporary file, read in the results, and then parsing them. A temporary file became necessary as *Words* paginates the stdout; writing to/reading from a file is as far as I can tell then the most efficient way to get batch results from *Words*. The read/write time likely adds some processing time, but *Words* still runs pretty fast.\n",
    "\n",
    "Appendix A offers assistance with installation and configuration of TreeTagger for OSX. \\[PJB 5.11.18\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "path = '/usr/local/bin/words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tools\n",
    "\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of CLTK tools\n",
    "\n",
    "tokenizer = WordTokenizer('latin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with *Words* and ```subprocess```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up subprocess commands\n",
    "\n",
    "token = 'verbum'\n",
    "cmd1 = f'echo {token}'\n",
    "cmd2 = 'xargs words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verb.um              N      2 2 NOM S N                 \n",
      "verb.um              N      2 2 VOC S N                 \n",
      "verb.um              N      2 2 ACC S N                 \n",
      "verbum, verbi  N (2nd) N   [XXXAX]  \n",
      "word; proverb; [verba dare alicui => cheat/deceive someone];\r\n",
      "*\n",
      "\n",
      "CPU times: user 7.81 ms, sys: 17.2 ms, total: 25 ms\n",
      "Wall time: 49.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Build subprocess pipe\n",
    "# NBL: I'll write up shlex and subprocess.PIPE at some point\n",
    "\n",
    "p1 = subprocess.Popen(shlex.split(cmd1), stdout=subprocess.PIPE)\n",
    "p2 = subprocess.Popen(shlex.split(cmd2), stdin=p1.stdout, stdout=subprocess.PIPE)\n",
    "result = p2.communicate()[0].decode()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['verbum']\n"
     ]
    }
   ],
   "source": [
    "lemmas = re.findall(r\"^.*?\\[.{5}\\].*?$\",result,re.MULTILINE)\n",
    "lemmas = [item.split(',')[0] for item in lemmas]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build a lemmatize function; use form frequency to return a single lemma\n",
    "\n",
    "def words_lemmatize(tokens):\n",
    "    from string import punctuation\n",
    "    text = '\\n'.join(tokens)\n",
    "    \n",
    "    os.makedirs('tmp', exist_ok=True)\n",
    "    \n",
    "    with open('tmp/tmp.txt','w+') as f:\n",
    "        f.write(text)\n",
    "    \n",
    "    cmd1 = 'words tmp/tmp.txt tmp/tmp_out.txt'\n",
    "\n",
    "    p1 = subprocess.run(shlex.split(cmd1))\n",
    "    \n",
    "    with open('tmp/tmp_out.txt') as f:\n",
    "        output = f.read()\n",
    "\n",
    "    results = output.strip().replace('*','\\n').split('\\n\\n')\n",
    "    result_lemmas = []\n",
    "    for result in results:\n",
    "        _lemmas = re.findall(r\"^\\w.*?\\[.....].*?$\", result, re.MULTILINE)\n",
    "        _lemmas = [re.split(r'[,| ]', item)[0] for item in _lemmas]\n",
    "        result_lemmas.append(_lemmas)\n",
    "    \n",
    "    lemmas = []\n",
    "\n",
    "    pos = 0\n",
    "    for token in tokens:\n",
    "        if token in punctuation or len(token) == 1:\n",
    "            lemmas.append(list(token))\n",
    "        elif result_lemmas[pos]:\n",
    "            lemmas.append(result_lemmas[pos])\n",
    "            pos += 1\n",
    "        else:\n",
    "            lemmas.append(None)\n",
    "            pos += 1\n",
    "    \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['carpo'], ['dies'], [','], ['quam', 'quam'], ['parvus'], ['credulus'], ['posterus', 'posterus']]\n",
      "CPU times: user 6.68 ms, sys: 10.8 ms, total: 17.5 ms\n",
      "Wall time: 55.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(words_lemmatize('carpe diem , quam minimum credula postero'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up sample text\n",
    "\n",
    "# Sall. Bell. Cat. 1\n",
    "text = \"\"\"Omnis homines, qui sese student praestare ceteris animalibus, summa ope niti decet, ne vitam silentio transeant veluti pecora, quae natura prona atque ventri oboedientia finxit. Sed nostra omnis vis in animo et corpore sita est: animi imperio, corporis servitio magis utimur; alterum nobis cum dis, alterum cum beluis commune est. Quo mihi rectius videtur ingeni quam virium opibus gloriam quaerere et, quoniam vita ipsa, qua fruimur, brevis est, memoriam nostri quam maxume longam efficere. Nam divitiarum et formae gloria fluxa atque fragilis est, virtus clara aeternaque habetur. Sed diu magnum inter mortalis certamen fuit, vine corporis an virtute animi res militaris magis procederet. Nam et, prius quam incipias, consulto et, ubi consulueris, mature facto opus est. Ita utrumque per se indigens alterum alterius auxilio eget.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.2 ms, sys: 10.8 ms, total: 27.9 ms\n",
      "Wall time: 226 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Tokenize and lemmatize sample text\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "lemmas = words_lemmatize(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Omnis', ['omnis', 'omne', 'omnis']), ('homines', ['homo']), (',', [',']), ('qui', ['queo', 'qui']), ('sese', None), ('student', ['studeo']), ('praestare', ['praesto', 'praesto', 'praesto']), ('ceteris', ['ceterus']), ('animalibus', ['animal', 'animalis', 'animalis']), (',', [',']), ('summa', ['summus', 'summa', 'summum']), ('ope', ['ops']), ('niti', ['nitor', 'nitor']), ('decet', ['decet']), (',', [',']), ('ne', ['neo', 'ne', 'ne']), ('vitam', ['vita']), ('silentio', ['silentium']), ('transeant', ['transeo']), ('veluti', ['veluti']), ('pecora', ['pecus']), (',', [',']), ('quae', None), ('natura', ['nascor', 'natura', 'naturo']), ('prona', ['pronus'])]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(tokens, lemmas))[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tagging performance on the entirety of Sallust's *Bellum Catilinum*\n",
    "\n",
    "from cltk.corpus.latin import latinlibrary\n",
    "bc = latinlibrary.raw('sall.1.txt')\n",
    "bc = bc[bc.find('[1]'):bc.find('Sallust The Latin Library The Classics Page')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script for preprocessing texts\n",
    "\n",
    "import html\n",
    "import re\n",
    "import string\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    replacer = JVReplacer()\n",
    "    \n",
    "    text = html.unescape(text) # Handle html entities\n",
    "    text = re.sub(r'&nbsp;?', ' ',text) #&nbsp; stripped incorrectly in corpus?\n",
    "    text = re.sub(r'\\x00',' ',text) #Another space problem?\n",
    "        \n",
    "    text = text.lower()\n",
    "    text = replacer.replace(text) #Normalize u/v & i/j    \n",
    "    \n",
    "    punctuation =\"\\\"#$%&\\'()*+,-/:;<=>@[\\]^_`{|}~.?!«»—\"\n",
    "    translator = str.maketrans({key: \" \" for key in punctuation})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    translator = str.maketrans({key: \" \" for key in '0123456789'})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    text = re.sub('[ ]+',' ', text) # Remove double spaces\n",
    "    text = re.sub('\\s+\\n+\\s+','\\n', text) # Remove double lines and trim spaces around new lines\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10802 tokens in Sallust's *Bellum catilinae*\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text\n",
    "\n",
    "bc = preprocess(bc)\n",
    "bc_tokens = tokenizer.tokenize(bc)\n",
    "print(f'There are {len(bc_tokens)} tokens in Sallust\\'s *Bellum catilinae*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 253 ms, sys: 30.3 ms, total: 283 ms\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "results = words_lemmatize(bc_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A: Help with installing Whitaker's *Words*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original Ada for Whitaker's Words is readily available in several places online. For OSX, the best option at this point is the binary provided by David Sanson. Installation from that point is pretty straightforward, esp. if you have been following the other posts in this lemmatization series.\n",
    "\n",
    "1. Download and unzip the Words binary\n",
    "    - [words-os-x-universal.zip](https://github.com/dsanson/Words/blob/master/binaries/MAC/words-os-x-universal.zip)\n",
    "2. Move the unzipped folder to ```/usr/local/bin```; a command like ```mv ./words /usr/local/bin``` should work.\n",
    "3. Change directory to ```/usr/local/bin/words```.\n",
    "3. You should be all set now—try it out with the following:\n",
    "    - ```echo verbum | xargs words```\n",
    "    - Output  \n",
    "    \n",
    "        ```\n",
    "        verb.um              N      2 2 NOM S N                 \n",
    "        verb.um              N      2 2 VOC S N                 \n",
    "        verb.um              N      2 2 ACC S N                 \n",
    "        verbum, verbi  N (2nd) N   [XXXAX]  \n",
    "        word; proverb; \\[verba dare alicui => cheat/deceive someone];\n",
    "        *\n",
    "        ```\n",
    "    \n",
    "Words should now work as expected in the Notebooks above. If you notice any problems with the installation instructions, please open an issue in this repo.—PJB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B: *open_words* results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'carpe',\n",
       "  'defs': [{'orth': ['carpa', 'carpae'],\n",
       "    'senses': ['carp', '(Erasmus)'],\n",
       "    'infls': [{'ending': 'e',\n",
       "      'pos': 'noun',\n",
       "      'form': {'declension': 'nominative',\n",
       "       'number': 'singular',\n",
       "       'gender': 'C'}},\n",
       "     {'ending': 'e',\n",
       "      'pos': 'noun',\n",
       "      'form': {'declension': 'vocative', 'number': 'singular', 'gender': 'C'}},\n",
       "     {'ending': 'e',\n",
       "      'pos': 'noun',\n",
       "      'form': {'declension': 'ablative', 'number': 'singular', 'gender': 'C'}},\n",
       "     {'ending': 'e',\n",
       "      'pos': 'verb',\n",
       "      'form': {'tense': 'present',\n",
       "       'voice': 'active',\n",
       "       'mood': 'imperative',\n",
       "       'person': 2,\n",
       "       'number': 'singular'}}]}]},\n",
       " {'word': 'diem',\n",
       "  'defs': [{'orth': ['dies', 'die'],\n",
       "    'senses': ['day',\n",
       "     'daylight',\n",
       "     '(sunlit hours)',\n",
       "     '(24 hours from midnight)',\n",
       "     'open sky',\n",
       "     'weather'],\n",
       "    'infls': [{'ending': 'em',\n",
       "      'pos': 'noun',\n",
       "      'form': {'declension': 'accusative',\n",
       "       'number': 'singular',\n",
       "       'gender': 'C'}}]}]}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ! pipenv install git+https://github.com/ArchimedesDigital/open_words.git#egg=open_words\n",
    "\n",
    "from open_words.parse import Parse\n",
    "\n",
    "parser = Parse()\n",
    "parser.parse_line('carpe diem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'posse',\n",
       " 'defs': [{'orth': ['posso', 'pote', 'potui', ''],\n",
       "   'senses': ['be able, can',\n",
       "    '[multum  posse => have much/more/most influence/power]'],\n",
       "   'infls': [{'ending': 'e',\n",
       "     'pos': 'verb',\n",
       "     'form': {'tense': 'present',\n",
       "      'voice': 'active',\n",
       "      'mood': 'infinitive',\n",
       "      'person': 0,\n",
       "      'number': ''}}]}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse('posse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'sunt',\n",
       " 'defs': [{'orth': ['sunt'],\n",
       "   'senses': ['to be, exist',\n",
       "    'also used to form verb perfect passive tenses with NOM PERF PPL'],\n",
       "   'infls': [{'form': {'form': 'PRES ACTIVE IND 3 P'},\n",
       "     'ending': '',\n",
       "     'pos': 'verb'}]}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse('sunt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
