{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LatMor-based Backoff Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what a [LatMor](#)-based sublemmatizer for the CLTK Latin Backoff Lemmatizer might look like. Just as in the earlier post on TreeTaggerBackoff, this creates a subclass of NLTK's SequentialBackoffTagger that gets lemma information by running *LatMor* via ```subprocess```. In order to return a single result from *LatMor*, I have used the weighting strategy from the earlier post; again, this is not a sound, longterm strategy, only a temporary one.\n",
    "\n",
    "The examples below are the same as in recent notebooks: 1. Sallust's *Bellum Catilinae* and the opening stanzas of a the nonsense Latin text of a [*Jabberwocky* translation](http://www.thelatinlibrary.com/iabervocius.html). The first is there to show general performance and the second to show the flexibility of the Backoff Lemmatizer.\n",
    "\n",
    "This backoff class requires *LatMor* and *SFST* to be installed.\\[PJB 5.6.18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import re\n",
    "import subprocess\n",
    "import shlex\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tag.sequential import SequentialBackoffTagger\n",
    "\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of CLTK tools\n",
    "\n",
    "tokenizer = WordTokenizer('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TreeTaggerLemmatizer as subclass of NLTK's Sequential Backoff Tagger\n",
    "\n",
    "from string import punctuation     \n",
    "\n",
    "class LatMorLemmatizer(SequentialBackoffTagger):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, backoff=None):\n",
    "        \"\"\"Setup for LatMorLemmatizer\"\"\"\n",
    "        # Add test blocks for installation of LatMor and SFST\n",
    "        SequentialBackoffTagger.__init__(self, backoff)\n",
    "        \n",
    "\n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        \"\"\"Returns a lemma for the token at a given index using LatMor\n",
    "        :param tokens: List of tokens to be lemmatized\n",
    "        :param index: Int with current token\n",
    "        :param history: List with tokens that have already been lemmatized\n",
    "        :return: String, spec. the lemma found at the current index.\n",
    "        \"\"\"             \n",
    "        if tokens[index] in punctuation: # LatMor does not handle punctuation; return punc immediately\n",
    "            return tokens[index]\n",
    "        \n",
    "        # Set up commands for subprocess.PIPE and build pipe\n",
    "        cmd1 = f'echo {tokens[index].lower()}'\n",
    "        cmd2 = '/usr/local/bin/fst-infl /usr/local/bin/latmor/latmor.a'\n",
    "        p1 = subprocess.Popen(shlex.split(cmd1), stdout=subprocess.PIPE)\n",
    "        p2 = subprocess.Popen(shlex.split(cmd2), stdin=p1.stdout, stdout=subprocess.PIPE)\n",
    "        \n",
    "        # Get the stdout with communicate\n",
    "        result = p2.communicate()[0].decode()\n",
    "        \n",
    "        # Parse the LatMor results\n",
    "        result_lines = result.split('\\n')\n",
    "        results = [r.split('<')[0] for r in result_lines if r.endswith('>')]\n",
    "        \n",
    "        return max(results,key=results.count) if results else None # TAKE THE HIGHEST FREQ LEMMAâ€”FIX!!!\n",
    "    \n",
    "    \n",
    "    def _batch_lemmatize(tokens):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def lemmatize(self, tokens):\n",
    "        # Setup global variable to build lemma list more quickly/efficiently?\n",
    "        # ^^^ use _batch_lemmatize\n",
    "        return self.tag(tokens) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up lemmatizer\n",
    "\n",
    "lemmatizer = LatMorLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "\n",
    "text = \"\"\"Omnis homines qui sese student praestare ceteris animalibus, summa ope niti decet, ne vitam silentio transeant veluti pecora, quae natura prona atque ventri oboedientia finxit. Sed nostra omnis vis in animo et corpore sita est: animi imperio, corporis servitio magis utimur; alterum nobis cum dis, alterum cum beluis commune est. Quo mihi rectius videtur ingeni quam virium opibus gloriam quaerere et, quoniam vita ipsa, qua fruimur, brevis est, memoriam nostri quam maxume longam efficere. Nam divitiarum et formae gloria fluxa atque fragilis est, virtus clara aeternaque habetur. Sed diu magnum inter mortalis certamen fuit, vine corporis an virtute animi res militaris magis procederet. Nam et, prius quam incipias, consulto et, ubi consulueris, mature facto opus est. Ita utrumque per se indigens alterum alterius auxilio eget.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 196 ms, sys: 487 ms, total: 683 ms\n",
      "Wall time: 6.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get lemmas\n",
    "\n",
    "lemma_pairs = lemmatizer.lemmatize(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Omnis', 'omnis'),\n",
      " ('homines', 'homo'),\n",
      " ('qui', 'qui'),\n",
      " ('sese', 'sese'),\n",
      " ('student', 'studere'),\n",
      " ('praestare', 'praestare'),\n",
      " ('ceteris', 'ceterus'),\n",
      " ('animalibus', 'animalis'),\n",
      " (',', ','),\n",
      " ('summa', 'summus')]\n"
     ]
    }
   ],
   "source": [
    "# Print sample\n",
    "\n",
    "pprint(lemma_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another sample text\n",
    "\n",
    "text = \"\"\" Est brilgum: tovi slimici\n",
    "In vabo tererotitant\n",
    "Brogovi sunt macresculi\n",
    "Momi rasti strugitant.\n",
    "\n",
    "\"Fuge Gabrobocchia, fili mi,\n",
    "Qui fero lacerat morsu:\n",
    "Diffide Iubiubae avi\n",
    "Es procul ab Unguimanu.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.3 ms, sys: 107 ms, total: 147 ms\n",
      "Wall time: 1.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get lemmas\n",
    "\n",
    "lemma_pairs = lemmatizer.lemmatize(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Est', 'esse'),\n",
      " ('brilgum', None),\n",
      " (':', ':'),\n",
      " ('tovi', None),\n",
      " ('slimici', None),\n",
      " ('In', 'in'),\n",
      " ('vabo', None),\n",
      " ('tererotitant', None),\n",
      " ('Brogovi', None),\n",
      " ('sunt', 'esse')]\n"
     ]
    }
   ],
   "source": [
    "# Print sample\n",
    "\n",
    "pprint(lemma_pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that only words like 'sunt' and 'in' (and punctuation) are lemmatized on a first pass. (Even *est*, correctly!) These words are again the functional vocabulary necessary to keep the nonsense Latin recognizable as Latin at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional lemmatizers/resources\n",
    "\n",
    "from cltk.lemmatize.backoff import UnigramLemmatizer, RegexpLemmatizer\n",
    "from cltk.lemmatize.latin.latin import latin_sub_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another sample text\n",
    "\n",
    "text = \"\"\" Est brilgum tovi slimici\n",
    "In vabo tererotitant\n",
    "Brogovi sunt macresculi\n",
    "Momi rasti strugitant\n",
    "\n",
    "\"Fuge Gabrobocchia fili mi\n",
    "Qui fero lacerat morsu\n",
    "Diffide Iubiubae avi\n",
    "Es procul ab Unguimanu\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display \n",
    "# ^^^ Ignore cell-specific warnings ^^^\n",
    "\n",
    "# Set up a more expansive backoff chain\n",
    "\n",
    "r = RegexpLemmatizer([\n",
    "    ('(.)(ant)$', '\\\\1o'), \n",
    "    ('(.)(um)$', '\\\\1us'),\n",
    "    ('(.)(i)$', '\\\\1us')\n",
    "])\n",
    "\n",
    "u = UnigramLemmatizer(model={'Momi': 'momus'}, backoff=r)\n",
    "r = RegexpLemmatizer(latin_sub_patterns, backoff=u)\n",
    "l = LatMorLemmatizer(backoff=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.8 ms, sys: 102 ms, total: 143 ms\n",
      "Wall time: 1.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get lemmas\n",
    "tokens = tokenizer.tokenize(text)\n",
    "lemma_pairs = l.lemmatize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Est', 'esse'),\n",
      " ('brilgum', 'brilgus'),\n",
      " ('tovi', 'tovus'),\n",
      " ('slimici', 'slimicus'),\n",
      " ('In', 'in'),\n",
      " ('vabo', 'vo'),\n",
      " ('tererotitant', 'tererotito'),\n",
      " ('Brogovi', 'Brogovus'),\n",
      " ('sunt', 'esse'),\n",
      " ('macresculi', 'macresculis')]\n"
     ]
    }
   ],
   "source": [
    "# Print sample\n",
    "\n",
    "pprint(lemma_pairs[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
