{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collatinus-based Backoff Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what a [Collatinus](#)-based sublemmatizer for the CLTK Latin Backoff Lemmatizer might look like. Just as in the earlier post on TreeTaggerBackoff, this creates a subclass of the sequential backoff tagger that gets lemma information from the *PyCollatinus* package. The ```lemmatize``` method takes a list of tokens (like all of the CLTK backoff lemmatizers), joins them, and runs this string through *PyCollatinus*. There is the additional step of reintroducing punctuation, which is ignored by the *PyCollatinus* ```Lemmatiseur``` class. WHAT TO DO WITH NONE. In order to return a single result from *PyCollatinus*, I have used the weighting strategy from the earlier post; again, this is not a sound, longterm strategy, only a temporary one.\n",
    "\n",
    "<!---\n",
    "When used on its own, the results are what you would expect from [TreeTagger](https://github.com/diyclassics/lemmatizer-experiments/blob/master/notebooks/latin-lemmatization-with-treetagger.ipynb). This is shown in the first example below using the first paragraph of Sallust's *Bellum Catilinae*.\n",
    "\n",
    "The next two examples—using the nonsense Latin text of a [*Jabberwocky* translation](http://www.thelatinlibrary.com/iabervocius.html)—shows the flexibility of the Backoff Lemmatizer and how the TreeTaggerLemmatizer results can improved upon.\n",
    "\n",
    "In the first example, we create a chain of TreeTaggerLemmatizer and RegexpLemmatizer and see that a nonsense word like *vabo* is reasonably(?) lemmatized to *vo*, because the Latin regex substitution patterns include *-abo > -o*. *Vabo* is the hapaxest of hapax legomena—it has never appeared in another Latin text and so will not be found in any lemma dictionary. (It is also admittedly wrong—*vabo* here is clearly the ablative of a noun *vabus* or *vabum* in the phrase *in vabo*. We will fix this in a future post by combining lemmatization with POS-tagging.)\n",
    "--->\n",
    "\n",
    "The next example, expands the backoff chain to include additional regex patterns and a custom lemma dictionary (using the UnigramLemmatizer) to lemmatize fully the first ten tokens in the *Jabberwocky*.\n",
    "\n",
    "This backoff class requires the *PyCollatinus* package.\\[PJB 5.6.18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tag.sequential import SequentialBackoffTagger\n",
    "\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "\n",
    "from pycollatinus import Lemmatiseur\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of CLTK tools\n",
    "\n",
    "tokenizer = WordTokenizer('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TreeTaggerLemmatizer as subclass of NLTK's Sequential Backoff Tagger\n",
    "\n",
    "class CollatinusLemmatizer(SequentialBackoffTagger):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, backoff=None):\n",
    "        \"\"\"Setup for TreeTaggerLemmatizer().\"\"\"\n",
    "        SequentialBackoffTagger.__init__(self, backoff)\n",
    "        self.tagger = Lemmatiseur()\n",
    "        self._lemmas = []\n",
    "        \n",
    "        \n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        \"\"\"Returns the lemma at the index in the _lemmas list created\n",
    "        by TreeTagger in lemmatize.\n",
    "        :param tokens: List of tokens to be lemmatized\n",
    "        :param index: Int with current token\n",
    "        :param history: List with tokens that have already been lemmatized\n",
    "        :return: String, spec. the lemma found at the current index.\n",
    "        \"\"\"\n",
    "        #print(tokens)\n",
    "        print('Coll')\n",
    "        return self._lemmas[index]    \n",
    "    \n",
    "    def lemmatize(self, tokens):\n",
    "        lemmas = []\n",
    "        text = \" \".join([token.lower() for token in tokens])\n",
    "        lemmas = []\n",
    "        for _, _, lemma in self.tagger.tag(text):\n",
    "            if lemma == '<unknown>':\n",
    "                lemmas.append(None)\n",
    "            else:\n",
    "                lemmas.append(lemma.split('|')[0])\n",
    "        self._lemmas = lemmas\n",
    "        return self.tag(tokens)\n",
    "    \n",
    "\n",
    "    def _choose_weighted_lemmas(self, results):\n",
    "        \"\"\"\"\"\"\n",
    "        # Extract lemmas\n",
    "        lemmas = []\n",
    "\n",
    "        for result in results:\n",
    "            _lemmas = []\n",
    "            for _result in result:\n",
    "                _lemmas.append(_result['lemma'])\n",
    "            lemmas.append(_lemmas)\n",
    "        \n",
    "        # Get weighted lemmas\n",
    "        weighted_lemmas = []\n",
    "\n",
    "        for lemma in lemmas:\n",
    "            c = Counter(lemma)\n",
    "            weights = [(i, c[i] / len(lemma) * 100.0) for i in c]\n",
    "            weighted_lemmas.append(weights)\n",
    "            \n",
    "        # Get max lemma\n",
    "        \n",
    "        lemma_max = []\n",
    "\n",
    "        for weighted_lemma in weighted_lemmas:\n",
    "            if weighted_lemma:\n",
    "                weight_max = max(weighted_lemma,key=lambda item:item[1])[0]\n",
    "            else:\n",
    "                weight_max = None\n",
    "            lemma_max.append(weight_max)\n",
    "            \n",
    "        return lemma_max\n",
    "\n",
    "    \n",
    "    def _align_tokens_lemmas(self, tokens, lemmas):\n",
    "        from string import punctuation\n",
    "\n",
    "        lemma_pairs = []\n",
    "\n",
    "        pos = 0\n",
    "        for token in tokens:\n",
    "            if token in punctuation:\n",
    "                lemma_pairs.append((token, token))\n",
    "            else:\n",
    "                lemma_pairs.append((token, lemmas[pos]))\n",
    "                pos += 1\n",
    "        \n",
    "        return lemma_pairs\n",
    "        \n",
    "    \n",
    "    def lemmatize(self, tokens):\n",
    "        lemmas = []\n",
    "        text = \" \".join([token.lower() for token in tokens])\n",
    "        results = self.tagger.lemmatise_multiple(text)\n",
    "        lemmas = self._choose_weighted_lemmas(results)\n",
    "        aligned_results = self._align_tokens_lemmas(tokens, lemmas)\n",
    "        tokens = [token for token, _ in lemma_pairs]\n",
    "        lemmas = [lemma for _, lemma in lemma_pairs]\n",
    "        self.tokens = tokens\n",
    "#         print(tokens)\n",
    "        self._lemmas = lemmas\n",
    "#         print(lemmas)\n",
    "        return self.tag(self.tokens) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display \n",
    "# ^^^ Ignore cell-specific warnings ^^^\n",
    "\n",
    "# Set up lemmatizer\n",
    "\n",
    "lemmatizer = CollatinusLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "\n",
    "text = \"\"\"Omnis homines qui sese student praestare ceteris animalibus, summa ope niti decet, ne vitam silentio transeant veluti pecora, quae natura prona atque ventri oboedientia finxit. Sed nostra omnis vis in animo et corpore sita est: animi imperio, corporis servitio magis utimur; alterum nobis cum dis, alterum cum beluis commune est. Quo mihi rectius videtur ingeni quam virium opibus gloriam quaerere et, quoniam vita ipsa, qua fruimur, brevis est, memoriam nostri quam maxume longam efficere. Nam divitiarum et formae gloria fluxa atque fragilis est, virtus clara aeternaque habetur. Sed diu magnum inter mortalis certamen fuit, vine corporis an virtute animi res militaris magis procederet. Nam et, prius quam incipias, consulto et, ubi consulueris, mature facto opus est. Ita utrumque per se indigens alterum alterius auxilio eget.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 70.1 ms, sys: 2.43 ms, total: 72.5 ms\n",
      "Wall time: 70.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get lemmas\n",
    "\n",
    "lemma_pairs = lemmatizer.lemmatize(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Omnis', 'omnis'),\n",
      " ('homines', 'homo'),\n",
      " ('qui', 'qui'),\n",
      " ('sese', 'se'),\n",
      " ('student', 'studeo'),\n",
      " ('praestare', 'praesto'),\n",
      " ('ceteris', 'ceteri'),\n",
      " ('animalibus', 'animalis'),\n",
      " (',', ','),\n",
      " ('summa', 'summus')]\n"
     ]
    }
   ],
   "source": [
    "# Print sample\n",
    "\n",
    "pprint(lemma_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another sample text\n",
    "\n",
    "text = \"\"\" Est brilgum: tovi slimici\n",
    "In vabo tererotitant\n",
    "Brogovi sunt macresculi\n",
    "Momi rasti strugitant.\n",
    "\n",
    "\"Fuge Gabrobocchia, fili mi,\n",
    "Qui fero lacerat morsu:\n",
    "Diffide Iubiubae avi\n",
    "Es procul ab Unguimanu.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.1 ms, sys: 276 µs, total: 14.3 ms\n",
      "Wall time: 14.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get lemmas\n",
    "\n",
    "lemma_pairs = lemmatizer.lemmatize(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Est', 'edo'),\n",
      " ('brilgum', None),\n",
      " (':', ':'),\n",
      " ('tovi', None),\n",
      " ('slimici', None),\n",
      " ('In', 'in'),\n",
      " ('vabo', None),\n",
      " ('tererotitant', None),\n",
      " ('Brogovi', None),\n",
      " ('sunt', 'sum')]\n"
     ]
    }
   ],
   "source": [
    "# Print sample\n",
    "\n",
    "pprint(lemma_pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that only words like 'sunt' and 'in' (and punctuation) are lemmatized on a first pass. (Also, *est*, though incorrectly!) These words are again the functional vocabulary necessary to keep the nonsense Latin recognizable as Latin at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional lemmatizers/resources\n",
    "\n",
    "from cltk.lemmatize.backoff import UnigramLemmatizer, RegexpLemmatizer\n",
    "from cltk.lemmatize.latin.latin import latin_sub_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display \n",
    "# ^^^ Ignore cell-specific warnings ^^^\n",
    "\n",
    "# Set up a more expansive backoff chain\n",
    "\n",
    "r = RegexpLemmatizer([\n",
    "    ('(.)(ant)$', '\\\\1o'), \n",
    "    ('(.)(um)$', '\\\\1us'),\n",
    "    ('(.)(i)$', '\\\\1us')\n",
    "])\n",
    "u = UnigramLemmatizer(model={'Momi': 'momus'}, backoff=r)\n",
    "backoff = RegexpLemmatizer(latin_sub_patterns, backoff=u)\n",
    "lemmatizer = CollatinusLemmatizer(backoff=backoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "CPU times: user 13 ms, sys: 1.06 ms, total: 14.1 ms\n",
      "Wall time: 13.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get lemmas\n",
    "\n",
    "lemma_pairs = lemmatizer.lemmatize(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Est', 'sum'),\n",
      " ('brilgum', 'brilgus'),\n",
      " (':', ':'),\n",
      " ('tovi', 'tovus'),\n",
      " ('slimici', 'slimicus'),\n",
      " ('In', 'in'),\n",
      " ('vabo', 'vo'),\n",
      " ('tererotitant', 'tererotito'),\n",
      " ('Brogovi', 'Brogovus'),\n",
      " ('sunt', 'sum')]\n"
     ]
    }
   ],
   "source": [
    "# Print sample\n",
    "\n",
    "pprint(lemma_pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all ten of the first ten words of the *Jabberwocky* translation have been lemmatized, plausibly, if not correctly (whatever correctly means in lemmatizing nonsense poetry.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TreeTaggerLemmatizer as subclass of NLTK's Sequential Backoff Tagger\n",
    "\n",
    "from treetagger import TreeTagger\n",
    "\n",
    "class TreeTaggerLemmatizer(SequentialBackoffTagger):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, backoff=None):\n",
    "        \"\"\"Setup for TreeTaggerLemmatizer().\"\"\"\n",
    "        SequentialBackoffTagger.__init__(self, backoff)\n",
    "        self.tagger = TreeTagger(language='latin') # Error trap to see if module is installed!\n",
    "        self._lemmas = []\n",
    "        \n",
    "        \n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        \"\"\"Returns the lemma at the index in the _lemmas list created\n",
    "        by TreeTagger in lemmatize.\n",
    "        :param tokens: List of tokens to be lemmatized\n",
    "        :param index: Int with current token\n",
    "        :param history: List with tokens that have already been lemmatized\n",
    "        :return: String, spec. the lemma found at the current index.\n",
    "        \"\"\"\n",
    "        print('TT')\n",
    "        return self._lemmas[index]    \n",
    "    \n",
    "    def lemmatize(self, tokens):\n",
    "        print(tokens)\n",
    "        lemmas = []\n",
    "        text = \" \".join([token.lower() for token in tokens])\n",
    "        lemmas = []\n",
    "        for _, _, lemma in self.tagger.tag(text):\n",
    "            if lemma == '<unknown>':\n",
    "                lemmas.append(None)\n",
    "            else:\n",
    "                lemmas.append(lemma.split('|')[0])\n",
    "        self._lemmas = lemmas\n",
    "        return self.tag(tokens)\n",
    "    \n",
    "    \n",
    "# Here's the deal—lemmatize is only called once at the beginning of the backoff. All other processing code needs to be in choose_tag and possibly tag!    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another sample text\n",
    "\n",
    "text = \"\"\" Est brilgum tovi slimici\n",
    "In vabo tererotitant\n",
    "Brogovi sunt macresculi\n",
    "Momi rasti strugitant\n",
    "\n",
    "\"Fuge Gabrobocchia fili mi\n",
    "Qui fero lacerat morsu\n",
    "Diffide Iubiubae avi\n",
    "Es procul ab Unguimanu\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display \n",
    "# ^^^ Ignore cell-specific warnings ^^^\n",
    "\n",
    "# Set up a more expansive backoff chain\n",
    "\n",
    "r = RegexpLemmatizer([\n",
    "    ('(.)(ant)$', '\\\\1o'), \n",
    "    ('(.)(um)$', '\\\\1us'),\n",
    "    ('(.)(i)$', '\\\\1us')\n",
    "])\n",
    "#u = UnigramLemmatizer(model={'Momi': 'momus'}, backoff=r)\n",
    "r = RegexpLemmatizer(latin_sub_patterns, backoff=u)\n",
    "t = TreeTaggerLemmatizer(backoff=None)\n",
    "lemmatizer = CollatinusLemmatizer(backoff=t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "Coll\n",
      "TT\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-cf0555fe41fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n# Get lemmas\\ntokens = tokenizer.tokenize(text)\\nlemma_pairs = lemmatizer.lemmatize(tokens)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/patrick/Envs/lemmatizer-experiments-O7drOCS6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/patrick/Envs/lemmatizer-experiments-O7drOCS6/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrick/Envs/lemmatizer-experiments-O7drOCS6/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-90d0fdabd90c>\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m#         print(lemmas)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrick/Envs/lemmatizer-experiments-O7drOCS6/lib/python3.6/site-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrick/Envs/lemmatizer-experiments-O7drOCS6/lib/python3.6/site-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mtag_one\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtagger\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_taggers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-132-cb8c005b2203>\u001b[0m in \u001b[0;36mchoose_tag\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \"\"\"\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lemmas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get lemmas\n",
    "tokens = tokenizer.tokenize(text)\n",
    "lemma_pairs = lemmatizer.lemmatize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Est', 'sum'),\n",
      " ('brilgum', 'brilgus'),\n",
      " (':', ':'),\n",
      " ('tovi', 'tovus'),\n",
      " ('slimici', 'slimicus'),\n",
      " ('In', 'in'),\n",
      " ('vabo', 'vo'),\n",
      " ('tererotitant', 'tererotito'),\n",
      " ('Brogovi', 'Brogovus'),\n",
      " ('sunt', 'sum')]\n"
     ]
    }
   ],
   "source": [
    "# Print sample\n",
    "\n",
    "pprint(lemma_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
