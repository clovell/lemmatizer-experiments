{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latin Lemmatization with LemLat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LemLat](http://www.ilc.cnr.it/lemlat/) is yet another lemmatizer and morphological tagger for Latin. It is lexicon-driven, built on a substantial base of the *Oxford Latin Dictionary*, Georges's *Ausführliches Lateinisch-Deutsches Handwörterbuch*, Gradenwitz's *Laterculi Vocum Latinarum*, and, added recently, the onomasticon from Forcellini's *Lexicon Totius Latinitatis*. As far as lexical coverage, it surpasses (from what I can tell) similar tools. It was developed in 1990 by Andrea Bozzi, Giuseppe Cappelli, and Nino Marinone, with substantial additions in 2002 by Bozzi, Marco Passarotti, and Paolo Ruffolo. Work continues through the CIRCSE Research Centre and the code for version 3.0 (which this Notebook uses) is hosted on [GitHub](https://github.com/CIRCSE/LEMLAT3).\n",
    "\n",
    "There is no Python wrapper for LemLat. As with [LatMor](https://github.com/diyclassics/lemmatizer-experiments/blob/master/notebooks/latin-lemmatization-with-latmor.ipynb), this notebook moves in that direction, using ```subprocess``` to generate command line results and then parsing them. In a future post, I will build a wrapper for LemLat that can be used with the CLTK Backoff Lemmatizer. (NB: This runs pretty slowly. I have some ideas for speeding it up, but those will have to wait for now.)\n",
    "\n",
    "Unlike other recent posts, I have abandoned (for the moment) the strategy of choosing a single lemma for each token. As will become apparent in the next phase of this series of lemmatizer review posts, the CLTK Backoff Lemmatizer will soon begin to incorporate weighted scores for lemmas based on all available information. Accordingly, there is no benefit to discarding possible (if incorrect) lemmas at this stage in the tagging process.\n",
    "\n",
    "The last section of this post offers assistance with installation and configuration of LemLat for OSX. \\[PJB 5.11.18\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install LemLat\n",
    "\n",
    "# # See last cell for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "path = '/usr/local/bin/lemlat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tools\n",
    "\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of CLTK tools\n",
    "\n",
    "tokenizer = WordTokenizer('latin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with *LatMor* and ```subprocess```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up subprocess commands\n",
    "\n",
    "tokens = 'carpe diem , quam minimum credula postero'.split()\n",
    "text = '\\n'.join([token.lower() for token in tokens])\n",
    "cmd1 = ['echo', text]\n",
    "cmd2 = './lemlat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.5 ms, sys: 9.66 ms, total: 12.2 ms\n",
      "Wall time: 4.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set up subprocess and pipe\n",
    "\n",
    "p1 = subprocess.Popen(cmd1, stdout=subprocess.PIPE)\n",
    "p2 = subprocess.Popen(shlex.split(cmd2), stdin=p1.stdout, stdout=subprocess.PIPE)\n",
    "output = p2.communicate()[0].decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse results\n",
    "\n",
    "# Split into entries by token\n",
    "results = re.split(r'A>', output.strip())[1:-1]\n",
    "\n",
    "# Split into lemma lists\n",
    "lemmas = [re.findall(r'LEMMA =+\\n\\s(\\w+)', result) for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['carpo', 'carpus'], ['dies', 'dies'], [], ['qui'], ['minimus'], ['credulus', 'credulus'], ['postero', 'posterus', 'posterum']]\n"
     ]
    }
   ],
   "source": [
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build a lemmatize function; use form frequency to return a single lemma\n",
    "\n",
    "def lemlat_lemmatize(tokens):\n",
    "    from string import punctuation\n",
    "    text = '\\n'.join([token.lower() for token in tokens])\n",
    "    cmd1 = ['echo', text]\n",
    "    p1 = subprocess.Popen(cmd1, stdout=subprocess.PIPE)\n",
    "    p2 = subprocess.Popen(shlex.split(cmd2), stdin=p1.stdout, stdout=subprocess.PIPE)\n",
    "    output = p2.communicate()[0].decode()\n",
    "    results = re.split(r'A>', output.strip())[1:-1]\n",
    "    lemmas = []\n",
    "    for i, result in enumerate(results):\n",
    "        if result:\n",
    "            form = tokens[i]\n",
    "            _lemmas = re.findall(r'LEMMA =+\\n\\s(\\w+)', result)\n",
    "            if _lemmas:\n",
    "                lemmas.append(_lemmas)\n",
    "            elif form in punctuation:\n",
    "                lemmas.append(list(form))\n",
    "            else:\n",
    "                lemmas.append(None)            \n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['carpo', 'carpus'], ['dies', 'dies'], [','], ['qui'], ['minimus'], ['credulus', 'credulus'], ['postero', 'posterus', 'posterum']]\n",
      "CPU times: user 4.04 ms, sys: 12.7 ms, total: 16.8 ms\n",
      "Wall time: 4.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(lemlat_lemmatize('carpe diem , quam minimum credula postero'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up sample text\n",
    "\n",
    "# Sall. Bell. Cat. 1\n",
    "text = \"\"\"Omnis homines, qui sese student praestare ceteris animalibus, summa ope niti decet, ne vitam silentio transeant veluti pecora, quae natura prona atque ventri oboedientia finxit. Sed nostra omnis vis in animo et corpore sita est: animi imperio, corporis servitio magis utimur; alterum nobis cum dis, alterum cum beluis commune est. Quo mihi rectius videtur ingeni quam virium opibus gloriam quaerere et, quoniam vita ipsa, qua fruimur, brevis est, memoriam nostri quam maxume longam efficere. Nam divitiarum et formae gloria fluxa atque fragilis est, virtus clara aeternaque habetur. Sed diu magnum inter mortalis certamen fuit, vine corporis an virtute animi res militaris magis procederet. Nam et, prius quam incipias, consulto et, ubi consulueris, mature facto opus est. Ita utrumque per se indigens alterum alterius auxilio eget.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.4 ms, sys: 11.3 ms, total: 30.7 ms\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Tokenize and lemmatize sample text\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "lemmas = lemlat_lemmatize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Omnis', ['omnis']),\n",
      " ('homines', ['homo', 'homo']),\n",
      " (',', [',']),\n",
      " ('qui', ['queo', 'quis', 'qui']),\n",
      " ('sese', ['se']),\n",
      " ('student', ['studeo']),\n",
      " ('praestare', ['praesto']),\n",
      " ('ceteris', ['ceterus']),\n",
      " ('animalibus', ['animalis', 'animal']),\n",
      " (',', [','])]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(zip(tokens, lemmas))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LemLat performance on larger text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tagging performance on the entirety of Sallust's *Bellum Catilinum*\n",
    "\n",
    "from cltk.corpus.latin import latinlibrary\n",
    "bc = latinlibrary.raw('sall.1.txt')\n",
    "bc = bc[bc.find('[1]'):bc.find('Sallust The Latin Library The Classics Page')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script for preprocessing texts\n",
    "\n",
    "import html\n",
    "import re\n",
    "import string\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    replacer = JVReplacer()\n",
    "    \n",
    "    text = html.unescape(text) # Handle html entities\n",
    "    text = re.sub(r'&nbsp;?', ' ',text) #&nbsp; stripped incorrectly in corpus?\n",
    "    text = re.sub(r'\\x00',' ',text) #Another space problem?\n",
    "        \n",
    "    text = text.lower()\n",
    "    text = replacer.replace(text) #Normalize u/v & i/j    \n",
    "    \n",
    "    punctuation =\"\\\"#$%&\\'()*+,-/:;<=>@[\\]^_`{|}~.?!«»—\"\n",
    "    translator = str.maketrans({key: \" \" for key in punctuation})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    translator = str.maketrans({key: \" \" for key in '0123456789'})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    text = re.sub('[ ]+',' ', text) # Remove double spaces\n",
    "    text = re.sub('\\s+\\n+\\s+','\\n', text) # Remove double lines and trim spaces around new lines\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10802 tokens in Sallust's *Bellum catilinae*\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text\n",
    "\n",
    "bc = preprocess(bc)\n",
    "bc_tokens = tokenizer.tokenize(bc)\n",
    "print(f'There are {len(bc_tokens)} tokens in Sallust\\'s *Bellum catilinae*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.34 ms, sys: 9.68 ms, total: 16 ms\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "results = lemlat_lemmatize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['omnis'], ['homo', 'homo'], [','], ['queo', 'quis', 'qui'], ['se'], ['studeo'], ['praesto'], ['ceterus'], ['animalis', 'animal'], [','], ['summo', 'summa', 'summus'], ['ope', 'ope', 'ops', 'opis', 'opis'], ['nitor'], ['decet'], [','], ['ne', 'ne', 'neo'], ['uita'], ['silentium', 'silentium'], ['transeo'], None, ['pecus'], [','], ['quis', 'qui'], ['natura', 'natura'], ['prono', 'pronus']]\n"
     ]
    }
   ],
   "source": [
    "print(results[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help with installing LemLat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The installation instructions for LemLat are available on the [GitHub](https://github.com/CIRCSE/LEMLAT3) README. What I offer here is primarily documentation of how I prefer to install LemLat on OSX.\n",
    "\n",
    "1. Download and unzip the ['embedded' version](https://github.com/CIRCSE/LEMLAT3/blob/master/bin/osx_embedded.tar.gz) of LemLat\n",
    "2. Rename and move the unzipped folder to ```/usr/local/bin```; a command like ```mv ./osx_embedded /usr/local/bin/lemlat``` should work.\n",
    "    - NB: I have not been able to run lemlat outside of ```/usr/local/bin```, so you may need to change directory to continue.\n",
    "3. You should be all set now—try it out with the following:\n",
    "    - ```echo laudat | ./lemlat```\n",
    "    - Output\n",
    "        - ```***************************************```\n",
    "        - ```LEMLAT: latin morphological lemmatizer *```\n",
    "        - ```***************************************/latmor.a\"...\n",
    "finished.```\n",
    "        - ...\n",
    "        - ```Form=laudat```\n",
    "        - etc.\n",
    "4. There are a number of options for input and output files discussed on the [GitHub](https://github.com/CIRCSE/LEMLAT3) README.\n",
    "    \n",
    "LemLat should now work as expected in the Notebooks above. If you notice any problems with the installation instructions, please open an issue in this repo.—PJB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
